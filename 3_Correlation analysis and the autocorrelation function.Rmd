---
title: "3_Correlation analysis and the autocorrelation function"
author: "IO"
date: "4/20/2022"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Correlation analysis and the autocorrelation function

Financial asset returns: Changes in price as a fraction of the initial price over a given time (e.g., a week)  

Taking the log of a stock option shows us the percentage of increase/decrease in the returns. If the stock goes from 1 to 2, this is a %100 increase, while if it goes from 2 to 3, this increase is %50. In linear analyses, these two have just 1 unit of increase. With the log analyses, we can see the relative value of the returns, especially important for long term trend analyses (like years).

```{r}
plot.ts(cbind(EuStockMarkets, log(EuStockMarkets)))
```

```{r}
# getting the daily returns
## Divide all rows except the first and the last one and subtract 1 from the values
returns <- EuStockMarkets[-1,] / EuStockMarkets[-1860,] - 1

# We'll see that this formula does the same thing as diff(log(x))

#convert the returns to time series
returns <- ts(returns, 
              start = c(1991, 130),
              frequency = 260)

# diff(log(EuStockMarkets)) is the log of returns
plot(cbind(returns, diff(log(EuStockMarkets))))

```

```{r}
colMeans(returns)

apply(returns,        #apply in the data
      MARGIN = 2,     #in columns (1 would be rows)
      FUN = var)      #the var function (variance)

apply(returns,
      MARGIN = 2,
      FUN = sd)
```

```{r results='hide'}
par(mfrow = c(2,2))
apply(returns, MARGIN = 2, FUN = hist, main = "", xlab = "Return")
```

```{r results='hide'}
par(mfrow = c(2,2))
apply(returns,
      MARGIN = 2,
      FUN = qqnorm,
      main = "")
```

```{r}
pairs(EuStockMarkets)
```

```{r}
plot(diff(log(EuStockMarkets)),
     main = "Log Daily Returns")
```

```{r}
pairs(diff(log(EuStockMarkets)),
      main = "Log Daily Returns")
```

## Covariance and correlation

**Covariance** means to what degree two variables sync their movement (e.g., increase or decrease) in the values. For time series analysis, this syncronization is investigated at certain time points. The covariance coefficient depends on the scale of the variables (i.e., how big or small the values are).  

**Correlation** is a standardized version of covariance which does not depend on the scale of the measurements in the variables. 

> Correlation = cov(stock_A, stock_B) / (sd(stock_A) * sd(stock_B))

![Covariance and scatterplot](3_Correlation analysis and the autocorrelation function_insertimage_3.png){width=50%}  

## Autocorrelation

Autocorrelation means correlation analysis is done for a large number of variables in an efficient way. Lag 1 autocorrelation means the correlation of a stock prices between all consecutive days (today vs yesterday, yesterday vs the-other-day,...). It can give us some information about whether a series is dependent on its past or not.

```{r}
# acf() gives the autocorrelation for each lag, plot can be set to F to get only the values
# Blue lines represent %95 confidence intervals

acf(EuStockMarkets[,1], lag.max = 500, plot = T)
```

# Autoregression

## The autoregressive (AR) model

AR model is arguably the most widely used time series model. It is like a simple linear regression but each observation is regressed on the previous observation. This makes them a simple first order recursion processes by regressing today's observation on yesterday's observation. Additionally, WN and RW models are special cases of AR models. 

AR recursion:

> Today = Slope * Yesterday + Constant + Noise

AR recursion, mean centered version (as used in R):

> (Today - Mean) = Slope * (Yesterday - Mean) + Noise

> $$(Y_{t} - \mu) = \phi * (Y_{t-1} - \mu) + \epsilon_{t}$$

$$\epsilon_{t}$$ is the white noise (WN) with a mean zero.

$$\phi$$ is the slope parameter valued between -1 and 1. 

If the AR process's slope $$\phi = 0$$, then $$Y_{t} = \mu + \epsilon_{t}$$ which is a WN process. If it is $$\phi \neq 0$$, then $$Y_{t}$$ depends on both the error $$\epsilon_{t}$$ and the previous observation $$Y_{t-1}$$. Large values of the slope $$\phi$$ lead to greater autocorrelation and negative values of $$\phi$$ result in oscillatory time series.

![(1 & 3) phi is ~1 and each observation is close to its neighbors, meaning they show strong persistance (not oscillating too much) in level (2) phi is negative, shows oscillatory patterns, (4) so much dependence, the series quickly diverging downward](3_Correlation analysis and the autocorrelation function_insertimage_4.png){width=80%}  

![Autocorrelations in the AR process, (1-3) different positive values of phi, the slope, slower autocorrelation decay/dwindle rate when phi is larger, (4) when phi is negative, the autocorrelation is alternating/oscillating but still autocorrelation is decaying/dwindling](3_Correlation analysis and the autocorrelation function_insertimage_5.png){width=80%}  
If the mean $$\mu = 0$$ and the slope $$\phi = 1$$, then $$Y_{t} = Y_{t-1} + \epsilon_{t}$$, which is Today = Yesterday + Noise. This is a RW process in which the $$Y_{t}$$ is not stationary.

### Simulating a autoregressive model

```{r}
# ar is the phi (the slope) of the time series model
AR_50 <- arima.sim(model = list(ar = .5),
                  n = 100)

AR_90 <- arima.sim(model = list(ar = .9),
                   n = 100)

AR_neg75 <- arima.sim(model = list(ar = -.75),
                   n = 100)

plot(cbind(AR_90, AR_50, AR_neg75),
     main = "As the slope (phi) decreases, oscilattion of the process increase \n and autocorrelation (continuity from one obs to the next) decreases")
```

We can check the autocorrelation for AR models with differing slopes, like the one above. We can see that higher the slope (phi) of the AR model, higher autocorrelation. Furthermore, the negative phi value creates an alternating pattern between positive and negative values.

```{r}
par(mfrow = c(2,2))
acf(AR_90)
acf(AR_50)
acf(AR_neg75)
```

Persistence is defined by a high correlation between an observation and its lag, while anti-persistence is defined by a large amount of variation between an observation and its lag. IN the AR model, when the slope (phi) parameter approaches 1, the persistance increases but the process reverts to its mean quickly. Also, the ACF decays to zero at a quick rate, indicating that values far in the past have little impact on future values of the process.

![D shows the highest degree of persistence, with a clear downward trend](3_Correlation analysis and the autocorrelation function_insertimage_6.png){width=50%}  

### Comparing the random walk (RW) and autoregressive (AR) models

RW is a special case of AR model, which has a slope (phi) value of 1. RW has the attribute of being non-stationary but it exhibits very strong persistence. The autocovariance function (ACF) decays to zero very slowly, meaning past values have a long lasting impact on current values.

```{r}
par(mfrow = c(3,2))
# Simulate and plot AR model with slope 0.9 
AR_90 <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(AR_90)
acf(AR_90)

# Simulate and plot AR model with slope 0.98
AR_98 <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(AR_98)
acf(AR_98)

# Simulate and plot RW model
RW <- arima.sim(model = list(order=c(0,1,0)), n = 200)
ts.plot(RW)
acf(RW) 
```

## AR model estimation and forecasting

We'll use the US inflation data from 1950 to 1990.

```{r}
par(mfrow = c(1,2))

inflation <- as.ts(Ecdat::Mishkin[,1])

ts.plot(inflation)

acf(inflation)
```

```{r}
# Fitting an AR model to the inflation data

AR_inflation <- arima(inflation,
                      order = c(1,  
                                0,
                                0))

print(AR_inflation)
```

The slope $$\phi$$ is 0.596, the mean $$\mu$$ is 3.974, and the sigma^2 estimate (the innovation variance) is 9.713.

AR fitted values:

The prediction estimates of today given yesterday

> $$\hat{Today} = \hat{Mean} + \hat{Slope} * (Yesterday - \hat{Mean})$$

> $$\hat{Y_{t}} = \hat{\mu} + \hat{\phi} * (Y_{t-1} - \hat{\mu})$$

Residuals of the fitted AR model:

Today (the observed values) - the estimates of today equals to error in the model's estimate. The residuals in the AR model are estimates of a WN process.

> $$\hat{\epsilon{t}} = Y_{t} - \hat{Y_{t}}$$

```{r}
# We find the residuals by applying the above formula, which is observed - fitted values

AR_inflation_residuals <- inflation - residuals(AR_inflation)


ts.plot(inflation)
points(AR_inflation_residuals, 
       tpye = "l",
       col = "red",
       lty = 2)
```

We see that the AR model explains the variation observed in the inflation time series, meaning it encompasses the residuals within the model variance.  

To predict one data point forward from the AR model, we can use the `predict()` function.

```{r}
predicted_infation <- predict(AR_inflation)
predicted_infation$pred[1]

# Predict 6 months ahead
predict(AR_inflation, n.ahead = 6)$pred
# The SE
predict(AR_inflation, n.ahead = 6)$se
```

```{r}
# Run to plot the Nile series plus the forecast and 95% prediction intervals
AR_fit <- arima(Nile, order  = c(1,0,0))
ts.plot(Nile, xlim = c(1871, 1980))
AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)
```

# Moving Average

## The simple moving average model

Time series model used to account for very short-run autocorrelation, which is a model with autocorrelation constructed from white noise (error)

Moving Average: The average (mean) of the current and the previous white noise

> $$Today = Mean + Noise + Slope * (Yesterday's Noise)$$

> $$Y_{t} = \mu + \epsilon_{t} + \theta * (\epsilon_{t-1})$$

* $$\mu$$ is the mean
* $$\theta$$ is the slope (the MA coefficient with a value between -1 and 1)
* $$\epsilon_{t}$$ is a mean zero WN process
* $$\sigma^2$$ is the WN variance

If the slope $$\theta = 0$$, then the model is simply a white noise process $$Y_{t} = \mu + \epsilon_{t}$$.  

If the slope $$\theta \neq 0$$, then today's model estimate depends on the current noise $$\epsilon_{t}$$ and the previous noise $$\epsilon_{t-1}$$, which makes it autocorrelated.  

Large values of the slope $$\theta$$ lead to greater autocorrelation
Negative values of the slope $$\theta$$ result in oscillatory time series. 

![(A and B show persistance in level, meaning each obs is close to its neighbors, but larger theta B has greater persistance (C) MA series with negative MA coefficients (theta) show alternating/oscillatory patterns (D) No autocorrelation, meaning it is just a WN)](3_Correlation analysis and the autocorrelation function_insertimage_7.png){width=50%}  

The slope $$\theta$$ determines the strength of the autocorrelation but only for one period, which means just for the lag 1. The polarity of the MA coefficient $$\theta$$ (i.e., positive or negative) determines this lag 1 autocorrelation's positiveness or negativeness. 

![](3_Correlation analysis and the autocorrelation function_insertimage_8.png)

## Simulating a simple moving average model

```{r}
MA_neg5 <- arima.sim(model = list(ma = -.5),
                     n = 100)

MA_5 <- arima.sim(model = list(ma = .5),
                     n = 100)

MA_9 <- arima.sim(model = list(ma = .9),
                     n = 100)

plot.ts(cbind(MA_neg5, MA_5, MA_9))
```

Looking at the autocorrelation values:

```{r}
par(mfrow = c(2,2))

acf(MA_neg5)
acf(MA_5)
acf(MA_9)
```

## Compare AR and MA models

AR and MA models both have the mean zero WN $$\epsilon$$ (error) parameter.

In the MA model, today's observation is regressed on yesterday's noise $$\epsilon_{t-1}$$

Today = Mean + Noise + Slope * Yesterday's Noise

> $$Y_{t} = \mu + \epsilon_{t} + \theta * \epsilon_{t-1}$$

MA model has autocorrelation only at lag 1

In the AR model, today's observation is regressed on yesterday's observation $$Y_{t-1}$$.

(Today - Mean) = Slope * (Yesterday - Mean) + Noise

> $$(Y - \mu) = \phi * (Y_{t-1} - \mu) + \epsilon_{t}$$

AR model has autocorrelation at many levels.

![](3_Correlation analysis and the autocorrelation function_insertimage_11.png)

The best way to assess the fit of AR or MA models to a time series, the information criteria like AIC `AIC(model)` and BIC `BIC(model)` is used.

![A shows short-run dependence but reverts quickly to the mean, so it must be the MA model. Series B and C are consistent with AR and RW, respectively. Series D does not show any clear patterns, so it must be the WN model.](3_Correlation analysis and the autocorrelation function_insertimage_10.png){width=50%}  

RW and AR models typically show large autocorrelation for many lags, but the ACF of an AR delays to zero more quickly than that of the RW. The MA ACF should have approximately zero autocorrelation at all lags greater than 1. The WN ACF should have approximately zero autocorrelation at all lags.

![A shows autocorrelation for the first lag only, which is consistent with the expectations of the MA model. Plot B shows dissipating autocorrelation across several lags, consistent with the AR model. Plot C is consistent with a RW model with considerable autocorrelation for many lags. Finally. Plot D shows virtually no autocorrelation with any lags, consistent with a WN model](3_Correlation analysis and the autocorrelation function_insertimage_12.png){width=50%}

